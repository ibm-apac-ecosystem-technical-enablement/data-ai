{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Hands-on: Question Answering by Querying Planning Analytics Data\n\n## Overview\n\nThis Jupyter Notebook provides an example of how to:\n\n1. Develop a Question Answering application that can answer user's question based on Planning Analytics data.\n\n2. Construct a One-Shot Prompt and pass the prompt to Large Language Model (LLM) to generate MDX statement based on user's question.\n\n3. Construct another Prompt and pass the retrieved tabular dataset to Large Language Model (LLM) to generate answer based on the PA data."}, {"metadata": {}, "cell_type": "code", "source": "# Install library\n%pip install TM1py", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import libraries\nimport json\nimport os\nimport random\nimport requests\nfrom TM1py.Services import TM1Service\nfrom TM1py.Exceptions import TM1pyException\nfrom TM1py.Utils.Utils import build_pandas_dataframe_from_cellset\nfrom TM1py.Utils.Utils import build_cellset_from_pandas_dataframe\n\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n\nfrom TM1py.Services import TM1Service\nfrom TM1py.Exceptions import TM1pyException", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Setup Connection to Planning Analytics\n\nTo get connected to Planning Analytics server, you need these informations:\n- address\n- port\n- user\n- password\n- namespace\n\nYour connection is successful when you see the *Server Name* and *Product Version* information."}, {"metadata": {}, "cell_type": "code", "source": "# Set up connection to Planning Analytics server\ntry:\n    with TM1Service(\n        address=\"<YOUR PA SERVER ADDRESS HERE>\",\n        port=<YOUR PA PORT NUMBER HERE>,\n        ssl=False,\n        user=\"pm\",\n        password=\"IBMDem0s\",\n        namespace=\"Harmony LDAP\"\n    ) as tm1:\n        print(\"Server Name:\", tm1.server.get_server_name())\n        print(\"Product Version:\",tm1.server.get_product_version())\n\n# Error Handling        \nexcept TM1pyException as e:\n    if e.status_code == 401:\n        print('Wrong credentials')\n    elif e.status_code == 404:\n        print('Wrong connection')\n    else:\n        print('Something else went wrong. Check error code:', str(e))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. User's Question\n\nUser's question in natural language."}, {"metadata": {}, "cell_type": "code", "source": "# User's question\nquestion = \"How many units of product 21002 did we sell in year 1 in organization 102 through channel 10?\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Configuring watsonx.ai\n\nThe following section defines the input to the Large Language Model (LLM).\nProvides the credential for watsonx.ai as indicated below\n\n1. `watsonx_project_id`: The watsonx.ai **Project ID** provided in watsonx.ai project -> Manage -> Project ID\n2. `api_key`: The **API Key** provided in IBM Cloud -> Manage -> API Key"}, {"metadata": {}, "cell_type": "code", "source": "project_id = os.environ[\"PROJECT_ID\"]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\nurl = \"https://us-south.ml.cloud.ibm.com\"\n\n# Replace with your watsonx project id (look up in the project Manage tab)\n#watsonx_project_id = \"<YOUR WATSONX.AI PROJECT ID HERE>\"\nwatsonx_project_id = project_id\n\n# Replace with your IBM Cloud key\napi_key = \"<YOUR IBM CLOUD API KEY HERE>\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Initialize the watsonx model\nmodel_init = None\n\n# Function for model to generate MDX statements\ndef get_model_mdx(model_type, max_tokens, min_tokens, decoding, stop_sequences):\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.STOP_SEQUENCES: stop_sequences\n    }\n    global model_init\n    if model_init is None:\n        model_init = Model(\n            model_id=model_type,\n            params=generate_params,\n            credentials={\n                \"apikey\": api_key,\n                \"url\": url\n            },\n            project_id=watsonx_project_id\n        )\n\n    return model_init", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Function for model to generate summary\ndef get_model (model_type, max_tokens, min_tokens, decoding, repetition_penalty):\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.REPETITION_PENALTY: repetition_penalty\n    }\n    global model_init\n    if model_init is None:\n        model_init = Model(\n            model_id=model_type,\n            params=generate_params,\n            credentials={\n                \"apikey\": api_key,\n                \"url\": url\n            },\n            project_id=watsonx_project_id\n        )\n\n    return model_init", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Creating Prompt to generate MDX statement to query data in PA\n\n- Construct a One-Shot Prompt and pass the prompt to Large Language Model (LLM) to generate MDX statement.\n- Based on user's question in natural language, LLM will generate the MDX statement.\n- The MDX statement will be used to query data in Planning Analytics."}, {"metadata": {}, "cell_type": "code", "source": "prompt_mdx = \"\"\"Create an MDX Statement for a Planning Analytics View to display the desired data\n\nInput:\nHow many units of product 21001 did we sell in year 2 in organization 101 through channel 10?\n\nOutput:\nSELECT {[Revenue].[Revenue].[Units Sold]} ON 0, {[product].[product].[21001]}*{DRILLDOWNMEMBER({[Year].[Year].[Y2]} , {[Year].[Year].[Y2]})}*{TM1SubsetToSet([Month].[Month],\"MY\",\"public\")} ON 1 FROM [Revenue] WHERE ([organization].[organization].[101], [Channel].[Channel].[10], [Version].[Version].[Actual])\n\nInput:\n\"\"\" + question + \"\"\"\n\nOutput:\n\"\"\"\nprint(prompt_mdx)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Model Parameter for LLM to generate MDX statement to query data in PA"}, {"metadata": {}, "cell_type": "markdown", "source": "The following block specifies the the specifics for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n\n1. **model_type** specifies the LLM being used. In the example below is the **codellama/codellama-34b-instruct-hf** model that is good at coding. You can change it to other coding models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other ones in a PoX and see if they will provide different results. Refer [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-api-model-ids.html?context=wx&audience=wdp), for exhaustive list of models supported.\n\n2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n\n3. **min_tokens** specifies the minimum number of output tokens.\n\n4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **Top-P** and **Top-K**). More information on these additional parameters can be found from the watsonx.ai Technical Sales Level 3 class (https://learn.ibm.com/course/view.php?id=13452).\n\n5. **stop_sequences** specifies sequences of tokens that, when encountered, will cause the model to stop generating further tokens. This is useful for controlling the end of the output and preventing the generation of unwanted or irrelevant content."}, {"metadata": {}, "cell_type": "code", "source": "# Set up watsonx model and parameters\nmodel_type = \"codellama/codellama-34b-instruct-hf\" # Coding model\nmax_tokens = 200\nmin_tokens = 10\ndecoding = DecodingMethods.GREEDY\nstop_sequences = [\"\\n\\n\"]\n\n# Get the watsonx model\nmodel_mdx = get_model_mdx(model_type, max_tokens, min_tokens, decoding, stop_sequences)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 6. LLM Inferencing to generate MDX statement to query data in PA"}, {"metadata": {}, "cell_type": "code", "source": "# Send a prompt to model\ngenerated_response = model_mdx.generate(prompt_mdx)\nresponse_mdx = generated_response['results'][0]['generated_text']\n\n# Print model response\nprint(\"--------------------------------- Generated response -----------------------------------\")\nprint(response_mdx)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 7. Query data in PA based on the AI-generated MDX statement"}, {"metadata": {}, "cell_type": "code", "source": "# Retrieve data from the Cube and View as a dataframe\n# Reference: https://code.cubewise.com/blog/getting-data-from-tm1-with-python/\ndata = tm1.cubes.cells.execute_mdx(mdx=response_mdx, private=False, use_compact_json=True)\ndf = build_pandas_dataframe_from_cellset(data, multiindex=False, sort_values=False)\ndf", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Convert the dataframe to markdown format\ndf = df.rename({'Revenue': 'KPI'}, axis=1)\ndf_md = df.to_markdown(index=False)\nprint(df_md)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 8. Model Parameter for LLM to generate MDX statement to query data in PA"}, {"metadata": {}, "cell_type": "markdown", "source": "The following block specifies the the specifics for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n\n1. **model_type** specifies the LLM being used. In the example below is the **ibm/granite-13b-instruct-v2** model. You can change it to other models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other ones in a PoX and see if they will provide different results. Refer [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-api-model-ids.html?context=wx&audience=wdp), for exhaustive list of models supported.\n\n2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n\n3. **min_tokens** specifies the minimum number of output tokens.\n\n4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **Top-P** and **Top-K**). More information on these additional parameters can be found from the watsonx.ai Technical Sales Level 3 class (https://learn.ibm.com/course/view.php?id=13452).\n\n5. **repetition_penalty** controls the model's tendency to repeat the same phrases or tokens in its output. A higher repetition penalty discourages the model from generating repetitive content, which can enhance the diversity and readability of the output. This is particularly useful for improving the quality of generated text by reducing redundancy."}, {"metadata": {}, "cell_type": "code", "source": "# Set up watsonx model and parameters\nmodel_type = 'ibm/granite-13b-instruct-v2'\nmax_tokens = 1000\n#min_tokens = 10\ndecoding = DecodingMethods.GREEDY\nrepetition_penalty = 1\n\n# Get the watsonx model\nmodel_anwser = get_model(model_type, max_tokens, min_tokens, decoding, repetition_penalty)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 9. Creating Prompt to generate MDX statement to query data in PA\n\n- Construct a One-Shot Prompt and pass the prompt to Large Language Model (LLM) to generate MDX statement.\n- Based on user's question in natural language, LLM will generate the MDX statement.\n- The MDX statement will be used to query data in Planning Analytics."}, {"metadata": {}, "cell_type": "code", "source": "prompt = \"\"\"<s>[INST] <<SYS>> Answer the question with the information contained in the following table. If the question is unanswerable, say 'unanswerable'. <</SYS>> Question: \"\"\" + question + df_md + '[/INST]'\n\nprint(prompt)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 10. LLM Inferencing to generate answer based on the data queried from PA"}, {"metadata": {}, "cell_type": "code", "source": "# Send a prompt to model\ngenerated_response = model_anwser.generate(prompt)\nresponse_text = generated_response['results'][0]['generated_text']\n\n# Print model response\nprint(\"--------------------------------- Generated response -----------------------------------\")\nprint(response_text)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}