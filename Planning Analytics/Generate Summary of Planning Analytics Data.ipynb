{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Hands-on: Generate Summary of Planning Analytics Data \n\n## Overview\n\nThis Jupyter Notebook provides an example of how to:\n\n1. Connect to Planning Analytics server and retrieve dataset from Cube and View as a dataframe using TM1py.\n\n2. Construt a prompt and pass the tabular dataset to Large Language Model (LLM) to generate a summary of the data."}, {"metadata": {}, "cell_type": "code", "source": "# Install library\n%pip install TM1py", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Import libraries\nimport json\nimport os\nimport requests\n\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n\nfrom TM1py.Services import TM1Service\nfrom TM1py.Exceptions import TM1pyException", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Setup Connection to Planning Analytics\n\nTo get connected to Planning Analytics server, you need these informations:\n- address\n- port\n- user\n- password\n- namespace\n\nYour connection is successful when you see the *Server Name* and *Product Version* information."}, {"metadata": {}, "cell_type": "code", "source": "# Set up connection to Planning Analytics server\ntry:\n    with TM1Service(\n        address='<YOUR PA SERVER ADDRESS HERE>,\n        port= <YOUR PA PORT NUMBER HERE>,\n        ssl=False,\n        user=\"pm\",\n        password=\"IBMDem0s\",\n        namespace='Harmony LDAP'\n    ) as tm1:\n        print(\"Server Name:\", tm1.server.get_server_name())\n        print(\"Product Version:\",tm1.server.get_product_version())\n\n# Error Handling        \nexcept TM1pyException as e:\n    if e.status_code == 401:\n        print('Wrong credentials')\n    elif e.status_code == 404:\n        print('Wrong connection')\n    else:\n        print('Something else went wrong. Check error code:', str(e))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Retrieve dataset from Cube and View\n\nDefine your existing Cube and View name and retrieve it as a dataframe, then convert to markdown."}, {"metadata": {}, "cell_type": "code", "source": "# Define Cube and View name to be retrieved\ncube_name = 'Revenue'\nview_name = 'Input'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Retrieve data from the Cube and View as a dataframe\n# Reference: https://code.cubewise.com/blog/getting-data-from-tm1-with-python/\ndf = tm1.cubes.cells.execute_view_dataframe(cube_name=cube_name, \n                                            view_name=view_name, \n                                            private=False)\n# Display the dataframe\ndf", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Convert dataframe to markdown\ndf_md = df.to_markdown(index=False)\nprint(df_md)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Creating Prompt\n\nConstruct your prompt which will be passed to Large Language Model (LLM)."}, {"metadata": {}, "cell_type": "code", "source": "# Create a prompt\nprompt = f\"Extract the key findings from the table regarding units sold and describe their development.\\n\\n{df_md}\"\nprint(prompt)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Configuring watsonx.ai\n\nThe following section defines the input to the Large Language Model (LLM)."}, {"metadata": {}, "cell_type": "markdown", "source": "Provides the credential for watsonx.ai as indicated below\n\n1. `watsonx_project_id` - The watsonx.ai **Project ID** provided in watsonx.ai project -> Manage -> Project id\n2. `api_key` - The **API Key** provided in IBM Cloud -> Manage -> API Key"}, {"metadata": {}, "cell_type": "code", "source": "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\nurl = \"https://us-south.ml.cloud.ibm.com\"\n\n# Replace with your watsonx project id (look up in the project Manage tab)\nwatsonx_project_id = \"<YOUR WATSONX.AI PROJECT ID HERE>\"\n\n# Replace with your IBM Cloud key\napi_key = \"<YOUR IBM CLOUD API KEY HERE>\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Initialize the watsonx model\nmodel_init = None\n\ndef get_model(model_type, max_tokens, min_tokens, decoding, temperature):#\n\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.TEMPERATURE: temperature,\n    }\n    global model_init\n    if model_init == None:\n        model_init = Model(\n            model_id=model_type,\n            params=generate_params,\n            credentials={\n                \"apikey\": api_key,\n                \"url\": url\n            },\n            project_id= watsonx_project_id\n            )\n\n    return model_init", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The following block specifies the parameters for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n\n1. **model_type** specifies the LLM being used. In the example below it is the llama-2-70b-chat model. You can change it to other models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other LLM in a PoX and see if they will provide different results. Refer [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-api-model-ids.html?context=wx&audience=wdp), for exhaustive list of models supported.\n\n2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n\n3. **min_tokens** specifies the minimum number of output tokens.\n\n4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **Top-P** and **Top-K**). More information on these additional parameters can be found from the watsonx.ai Technical Sales Level 3 class (https://learn.ibm.com/course/view.php?id=13452).\n\n5. **temperature** specifies how conservative or creative the model will be. The lower it is, the more conservative it it. The range is from 0 to 2."}, {"metadata": {}, "cell_type": "code", "source": "# Set up watsonx model and parameters\nmodel_type = \"meta-llama/llama-2-70b-chat\"\nmax_tokens = 1000\nmin_tokens = 50\ndecoding = DecodingMethods.GREEDY\ntemperature = 0.7\n\n# Get the watsonx model\nmodel = get_model(model_type, max_tokens, min_tokens, decoding, temperature)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 5. Summary Generation\n\nThis block generates a summary based on the input prompt and the specified parameters."}, {"metadata": {}, "cell_type": "code", "source": "# Send a prompt to model\ngenerated_response = model.generate(prompt)\nresponse_text = generated_response['results'][0]['generated_text']\n\n# Print model response\nprint(\"--------------------------------- Generated response -----------------------------------\")\nprint(response_text)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}