{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbc29d5-76b4-4c9c-abdd-eaf8b4c00ebf",
   "metadata": {},
   "source": [
    "# Load Document Data from Web into Hive Catalog in watsonx.data (Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8a6903-4e80-416a-a4f5-082906630f89",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This Jupyter Notebook provides a step-by-step guide on how to prepare Web document data for RAG using Milvus as a vector database (in watsonx.data).\n",
    "\n",
    "In this notebook, we will prepare document data from the Web (Wikipedia) using a URL and populate it into the Hive Catalog (in watsonx.data). Here are the steps:\r",
    "1. Install and import libraries.\n",
    "2. Fetch web articles (Wikipedia articles) and populate into dataframe.\n",
    "3. Connect to watsonx.data.\n",
    "4. Create Schema and Table in Hive Catalog.\n",
    "5. Chunk the web documents and load into Hive Table.\n",
    "6. Check the loaded documents data in Hive Table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42caa0db-566a-4092-af10-7636456504b3",
   "metadata": {},
   "source": [
    "- Author: ahmad.muzaffar@ibm.com (APAC Ecosystem Technical Enablement).\n",
    "- This material has been adopted from material originally produced by Katherine Ciaravalli, Ken Bailey and George Baklarz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ae50a6",
   "metadata": {},
   "source": [
    "## 1. Install and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb1010b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install python-dotenv\n",
    "!pip install wikipedia\n",
    "!pip install pymilvus\n",
    "!pip install sentence_transformers\n",
    "!pip install grpcio==1.60.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e1e53-44ce-4057-93a7-f536ad8bea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb504b-97a1-4416-80c8-2ce2ef538cd2",
   "metadata": {},
   "source": [
    "## 1. Fetch web articles (Wikipedia articles) and populate into dataframe\n",
    "\n",
    "This notebook walks through the process of loading a wikipedia article into a watsonx.data relational database table. We use the [Wikipedia python library](https://pypi.org/project/wikipedia/) to retrieve wikipedia articles. We then create a table in the database to store the articles. Finally, we load the articles into the database. \n",
    "\n",
    "For details on the copyright issues when extracting data, please refer to the [Wikipedia Copyrights](https://en.wikipedia.org/wiki/Wikipedia:Copyrights) page.\n",
    "\n",
    "The following code will search Wikipedia articles and display a list of the articles by title. The initial search will return a list of up to 10 titles, while the subsequent call will retrieve the summary of the article. The two results are combined into one dataframe for easy scrolling.\n",
    "\n",
    "Update the next field to include what you are searching for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdc0fd-a213-485a-bf7f-8e7f5660bc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"climate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a03287-ba9a-4961-a8e9-2c906c565fb0",
   "metadata": {},
   "source": [
    "### Retrieve 10 Articles\n",
    "The next call will retrieve a maximum of 10 titles and display the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc02a33-de56-4b10-85cb-5f76971a202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = wikipedia.search(topic)\n",
    "print(\"Article Title\")\n",
    "print(\"-------------------------------------------------\")\n",
    "for result in search_results: print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f90df-2bea-4831-b5db-aa8d6eb43fef",
   "metadata": {},
   "source": [
    "### Retrieve Article Summary\n",
    "Now that we have a list of articles, we can request a summary of each article and display them. Note that if an article is ambiguous, the program will not attempt to retrieve the article. An ambiguous article is an article which could refer to multiple topics. The summary output from an ambiguous article will display possible searches that you may want to try. Since we are only interested in direct articles, the ambiguous titles will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99582e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# search\n",
    "search_results = wikipedia.search(\"Climate\")\n",
    "\n",
    "display_articles = []\n",
    "for i in range (0,len(search_results)):\n",
    "    try:\n",
    "        summary = wikipedia.summary(search_results[i])\n",
    "    except Exception as err:\n",
    "        print(f\"Skipped article '{search_results[i]}' skipped because of ambiguity.\")\n",
    "        continue\n",
    "        \n",
    "    display_articles.append({\n",
    "        \"title\"   : search_results[i],\n",
    "        \"summary\" : summary\n",
    "    })\n",
    "\n",
    "#print(display_articles)\n",
    "\n",
    "df = pd.DataFrame.from_dict(display_articles)\n",
    "df.style.set_properties(**{'text-align': 'left'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96792953",
   "metadata": {},
   "source": [
    "This step will load selected articles into watsonx.data. Since we are only interested in climate change, we will select the first two articles in the list. You can change the documents loaded by changing the document indexes in the variable found in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b436a408-9066-42d1-8c2d-8748bd79ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45505e75-ce12-49af-8998-2e6a649b5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch wikipedia articles\n",
    "articles = {}\n",
    "for document in documents:\n",
    "    articles.update({display_articles[document][\"title\"] : None})\n",
    "\n",
    "for k,v in articles.items():\n",
    "    article = wikipedia.page(k)\n",
    "    articles[k] = article.content\n",
    "    print(f\"Successfully fetched article {k}\")\n",
    "\n",
    "print(f\"Successfully fetched {len(articles)} articles \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b70849-6f29-41dd-ad9d-2ea7acffd9bd",
   "metadata": {},
   "source": [
    "## 2. Connect to watsonx.data\n",
    "The following code will use the Presto Magic commmands to load data in watsonx.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f6cde-bd30-4987-b5f8-2c6ebda05c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run presto.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861981f3-f699-4009-9c7a-ec645899e776",
   "metadata": {},
   "source": [
    "The connection details should not change unless you are attempting to run this script from a Jupyter environment that is outside of the developer system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae4fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "   connect\n",
    "   userid=ibmlhadmin\n",
    "   password=password\n",
    "   hostname=watsonxdata\n",
    "   port=8443\n",
    "   catalog=tpch\n",
    "   schema=tiny\n",
    "   certfile=/certs/lh-ssl-ts.crt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723dc88-a7a3-4931-aa16-eb33e679a1be",
   "metadata": {},
   "source": [
    "## 3. Create Schema and Table in Hive Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb54ddf5-e999-4912-a1c2-652a6951ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS hive_data.rag_web.web_wikipedia;\n",
    "DROP SCHEMA IF EXISTS hive_data.rag_web;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5107e0-d43a-4e93-a0c1-7c27c68ff6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step will delete any existing data in the rag_web bucket. \n",
    "# A DROP table command does not remove the files in the bucket. \n",
    "# You may see error messages displayed if no data or bucket exists.\n",
    "\n",
    "minio_host    = \"watsonxdata\"\n",
    "minio_port    = \"9000\"\n",
    "hive_host     = \"watsonxdata\"\n",
    "hive_port     = \"9083\"\n",
    "\n",
    "hive_id           = None\n",
    "hive_password     = None\n",
    "minio_access_key  = None\n",
    "minio_secret_key  = None\n",
    "keystore_password = None\n",
    "\n",
    "try:\n",
    "    with open('/certs/passwords') as fd:\n",
    "        certs = fd.readlines()\n",
    "    for line in certs:\n",
    "        args = line.split()\n",
    "        if (len(args) >= 3):\n",
    "            system   = args[0].strip()\n",
    "            user     = args[1].strip()\n",
    "            password = args[2].strip()\n",
    "            if (system == \"Minio\"):\n",
    "                minio_access_key = user\n",
    "                minio_secret_key = password\n",
    "            elif (system == \"Thrift\"):\n",
    "                hive_id = user\n",
    "                hive_password = password\n",
    "            elif (system == \"Keystore\"):\n",
    "                keystore_password = password\n",
    "            else:\n",
    "                pass\n",
    "except Error as e:\n",
    "    print(\"Certificate file with passwords could not be found\")\n",
    "\n",
    "%system mc alias set watsonxdata http://{minio_host}:{minio_port} {minio_access_key} {minio_secret_key}\n",
    "\n",
    "%system mc rm --recursive --force watsonxdata/hive-bucket/rag_web"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb69f3-e0bb-4697-a017-026ffb30e6b0",
   "metadata": {},
   "source": [
    "#### Create Schema (rag_web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eec943-b859-4c26-90fc-543be1fe1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE SCHEMA IF NOT EXISTS \n",
    "  hive_data.rag_web\n",
    "WITH (location = 's3a://hive-bucket/rag_docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1567f-6c6e-4845-8f51-dba6225c2986",
   "metadata": {},
   "source": [
    "#### Create Table (web_wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf175c18-9add-4146-8832-d4a1993c6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE hive_data.rag_web.web_wikipedia\n",
    "  (\n",
    "    \"id\" varchar,\n",
    "    \"text\" varchar,\n",
    "    \"title\" varchar  \n",
    "  )\n",
    "WITH \n",
    "  (\n",
    "  format = 'PARQUET',\n",
    "  external_location = 's3a://hive-bucket/rag_web' \n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3c109b-5f1b-4f80-9525-0f4daa4755e8",
   "metadata": {},
   "source": [
    "## 4. Chunk the web documents and load into the Table\n",
    "The Wikipedia article is written into the watsonx.data database in chucks of approximately 225 words in size. The reason for chunking the data is to make it more efficient when populating the Milvus system from watsonx.data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk data\n",
    "def split_into_chunks(text, chunk_size):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "split_articles = {}\n",
    "for k,v in articles.items():\n",
    "    split_articles[k] = split_into_chunks(v, 225)\n",
    "\n",
    "# Insert data\n",
    "for article_title, article_chunks in split_articles.items():\n",
    "    for i, chunk in enumerate(article_chunks):\n",
    "        escaped_chunk = chunk.replace(\"'\", \"''\").replace(\"%\", \"%%\")\n",
    "        insert_stmt = f\"insert into hive_data.rag_web.web_wikipedia values ('{i+1}', '{escaped_chunk}', '{article_title}')\"\n",
    "        %sql --quiet {insert_stmt}\n",
    "        print(f\"{article_title} {i+1}/{len(article_chunks)} inserted\",end=\"\\r\")\n",
    "            \n",
    "    print(f\"\\n{article_title} Insertion complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a347b2-7206-4d1c-bd79-e2793a04ba8d",
   "metadata": {},
   "source": [
    "## 5. Check the loaded documents data in the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "   SELECT * FROM hive_data.rag_web.web_wikipedia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
