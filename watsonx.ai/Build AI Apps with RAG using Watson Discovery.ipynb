{"cells": [{"metadata": {}, "id": "26bfa626", "cell_type": "markdown", "source": "# Hands-on: Build AI Apps with Retrieval-Augmented Generation (RAG) using watsonx.ai & Watson Discovery\n\n## Overview\n\nThis Jupyter Notebook provides an example of how to:\n\n1. Create a Watson Discovery collection and upload documents to it.\n\n2. Customize this notebook to perform a simple RAG exercise.\n\nThis source code performs the **Retrieval** task from the document(s) in the Watson Discovery collection. The retrieved information together with the prompt are passed to the Large Language Model (LLM) of your choice (as named in the Notebook) to generate the final result."}, {"metadata": {}, "id": "b1345eb9", "cell_type": "code", "source": "# Install library\n!pip install --upgrade ibm-watson", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "39e97e07", "cell_type": "code", "source": "# Import libraries\nimport json\nimport os\n\nfrom ibm_watson import DiscoveryV2\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n\n# WML python SDK\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "1d5b8f3e", "cell_type": "markdown", "source": "## 1. Watson Discovery set up\n\nWhen you set up Watson Discovery, you should have saved the credentials in a file called **ibm-credentials.env**. You will need to use the values from that file. You can open the file using a simple text editor.\n\n1. Find the value for **DISCOVERY_APIKEY** from the file and paste it as the value for **IAMAuthenticator** below (between the 2 single quotes).\n\n2. Find the value for **DISCOVERY_URL** from the file and paste it as the value for **discovery.set_service_url** below (between the 2 single quotes).\n\nThis initializes a connection to a Watson Discovery instance with a preloaded pdf document (the IBM Annual Report 2022).\n"}, {"metadata": {}, "id": "94ab9f92", "cell_type": "code", "source": "#Set up Watson Discovery credentials\nauthenticator = IAMAuthenticator('<YOUR WATSON DISCOVERY API KEY HERE>') # DISCOVERY_APIKEY  \ndiscovery = DiscoveryV2(\n    version='2020-08-30',\n    authenticator=authenticator\n)\n\ndiscovery.set_service_url('<YOUR WATSON DISCOVERY URL HERE>') # DISCOVERY_URL", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e174c40e", "cell_type": "markdown", "source": "## 2. Watson Discovery Search\n\nThis is a simple question (prompt) that is being posted to the model. This can be surfaced in a Streamlit GUI - which is not the focus of the lab. Clients may have other GUI tools. Here we focus on the underlying Watson Discovery, and later on watsonx.ai.\n"}, {"metadata": {}, "id": "eba2a135", "cell_type": "code", "source": "# question = 'I\u2019m interested in IBM\u2019s effect on the environment. What efforts have they been making in sustainability?'", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "23a3a855", "cell_type": "code", "source": "question = 'I\u2019m interested in IBM\u2019s initiatives on the business in AI. What efforts have they been making in AI?'", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "5d4fca9c", "cell_type": "code", "source": "#question = 'What is IBM net profit and revenue in 2022?'", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "1a00d2fe", "cell_type": "code", "source": "#question = 'For the year ended December 31, how much is Total revenue for Year 2021 and Year 2022. How much are the difference in USD and in %?' #Page 17", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "f3023c15", "cell_type": "markdown", "source": "For the block below, you will need to provide the proper information from the Watson Discovery project you created.\n\n1. The **Project ID**, paste the value in for **project_id** below (between the 2 single quotes).\n\n2. The **Collection ID** (for the collection that includes the IBM Annual Report 2022 report), paste the value in for **collection_ids** below (between the 2 single quotes).\n\nThere are a few parameters defined for Watson Discovery Search:\n\n- **passages.enabled**: A Boolean that specifies whether the service returns a set of the most relevant passage from the documents that were returned by a query that uses the natural_language_query parameter. Watson Discovery uses sophisticated algorithms to determine the best passages of text from all of the documents that are returned by a query. They are displayed as a section within each document query result and are ordered by passage relevance. Including passage retrieval in queries increases the response time because it takes more time to score the passages.\n\n- **passages.max_per_document**: One passage is returned per document by default. You can increase the maximum number of passages to return per document by specifying a higher number in the passages.max_per_document parameter.\n\n- **find_answers**: By default, Watson Discovery provides answers by returning the entire passage that contains the answer to a natural language query. When the answer-finding feature is enabled, Watson Discovery also provides a \"short answer\" within the passage, and a confidence score to show whether the \"short answer\" answers the question that is explicit or implicit in the user query.\n\n- **natural_language_query**: Use a natural language query to enter queries that are expressed in natural language, as might be received from a user in a conversational or free-text interface, such as IBM Watson Assistant. The parameter uses the entire input as the query text. It does not recognize operators. The maximum query string length for a natural language query is 2048.\n\nFor more details on the query parameters, see https://cloud.ibm.com/docs/discovery-data?topic=discovery-data-query-parameters.\n"}, {"metadata": {}, "id": "ba64b96d", "cell_type": "code", "source": "# Utilize the IBM Watson Discovery service to query a collection for information based on a natural language query\nresponse = discovery.query(\n  project_id='<YOUR WATSON DISCOVERY PROJECT ID HERE>',\n  collection_ids = ['<YOUR WATSON DISCOVERY COLLECTION ID HERE>'],\n  passages = {'enabled': True, \n              'max_per_document': 5,\n             'find_answers': True},\n  natural_language_query = question\n).get_result()\n\nwith open('data.json', 'w') as f:\n    json.dump(response, f)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "cae6b7c9", "cell_type": "markdown", "source": "The next 4 blocks provide some parsing for the output. You should not need to update these.\n"}, {"metadata": {}, "id": "60e7f453", "cell_type": "code", "source": "# Inspecting the key fields in the WD output\nresponse.keys()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "787e54bf", "cell_type": "code", "source": "# Only one relevant document (because one document in the collection)\nlen(response['results'])", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "f399b32a", "cell_type": "code", "source": "# Removing some tags\npassages = response['results'][0]['document_passages']\npassages = [p['passage_text'].replace('<em>', '').replace('</em>', '').replace('\\n','') for p in passages]\npassages", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e662b1e2", "cell_type": "code", "source": "# Concatenating passages\ncontext = '\\n '.join(passages)\ncontext", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2c5199df", "cell_type": "markdown", "source": "## 3. Creating Prompt\n\nThis section creates a prompt with instructions and context to allow the LLM to generate answers based on the passages retrieved by Watson Discovery, and on the rules specified below.\n\n- `<s>` `</s>`: Indicates the start and end of a sequence.\n- `[INST]` `[/INST]`: Suggests an instruction or command is being provided.\n- `<<SYS>>` `<</SYS>>`: Denotes the beginning of a system message, which in this case is giving specific instructions for how to process the input text.\n\nReference: https://huggingface.co/blog/llama2#how-to-prompt-llama-2\n"}, {"metadata": {}, "id": "92c047d1", "cell_type": "code", "source": "prompt = \\\n\"<s>[INST] <<SYS>> \\\nPlease answer the following question in one sentence using this text. \\\nIf the question is unanswerable, say 'unanswerable'. \\\nIf you responded to the question, don't say 'unanswerable'. \\\nDo not include information that's not relevant to the question. \\\nDo not answer other questions. \\\nMake sure the language used is English.v\\\nDo not use repetitions. <</SYS>>' \\\nQuestion:\" + question \\\n+ context + '[/INST]'\n\nprint(\"----------------------------------------------------------------------------------------------------\")\nprint(\"*** Prompt:\" + prompt + \"***\")\nprint(\"----------------------------------------------------------------------------------------------------\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "15f237cb", "cell_type": "markdown", "source": "## 4. Configuring watsonx.ai\n\nThe following section defines the input to the Large Language Model (LLM).\n"}, {"metadata": {}, "id": "fdf44eba", "cell_type": "markdown", "source": "Provides the credential for watsonx.ai as indicated below\n\n1. `watsonx_project_id` - The watsonx.ai **Project ID** provided in watsonx.ai project -> Manage -> Project id\n2. `api_key` - The **API Key** provided in IBM Cloud -> Manage -> API Key"}, {"metadata": {}, "id": "235e0b9e", "cell_type": "code", "source": "# URL of the hosted LLMs is hardcoded because at this time all LLMs share the same endpoint\nurl = \"https://us-south.ml.cloud.ibm.com\"\n\n# Replace with your watsonx project id (look up in the project Manage tab)\nwatsonx_project_id = \"<YOUR WATSONX.AI PROJECT ID HERE>\"\n\n# Replace with your IBM Cloud key\napi_key = \"<YOUR IBM CLOUD API KEY HERE>\"", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "f2048b47", "cell_type": "code", "source": "model_init = None\n# Initialize the watsonx model\ndef get_model(model_type,max_tokens,min_tokens,decoding,temperature):#, repetition_penalty):\n\n    generate_params = {\n        GenParams.MAX_NEW_TOKENS: max_tokens,\n        GenParams.MIN_NEW_TOKENS: min_tokens,\n        GenParams.DECODING_METHOD: decoding,\n        GenParams.TEMPERATURE: temperature,\n    }\n    global model_init\n    if model_init == None:\n        model_init = Model(\n            model_id=model_type,\n            params=generate_params,\n            credentials={\n                \"apikey\": api_key,\n                \"url\": url\n            },\n            project_id= watsonx_project_id\n            )\n\n    return model_init", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d9779123", "cell_type": "markdown", "source": "The following block specifies the the specifics for the LLM. In a PoX, you may want to vary these values to show a client how they can get the best results.\n\n1. **model_type** specifies the LLM being used. In the example below it is the llama-2-70b-chat model. You can change it to other models. Note that the size of the model will have implications on resource usage. You may wish to try some of the other ones in a PoX and see if they will provide different results. In the block below, there are 4 models (with 3 commented out, so llama2 is being used - you can comment out different ones to try). Refer [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-api-model-ids.html?context=wx&audience=wdp), for exhaustive list of models supported.\n\n2. **max_tokens** specifies the maximum number of output tokens. Keep in mind that 1 token does not equal 1 word. In general, you can estimate roughly 3 tokens per word.\n\n3. **min_tokens** specifies the minimum number of output tokens.\n\n4. **decoding** specifies the decoding method. You can also choose to do **sampling** decoding - in which case you can specify more parameters (such as **Top-P** and **Top-K**). More information on these additional parameters can be found from the Watsonx.ai Technical Sales Level 3 class (https://learn.ibm.com/course/view.php?id=13452).\n\n5. **temperature** specifies how conservative or creative the model will be. The lower it is, the more conservative it it. The range is from 0 to 2."}, {"metadata": {}, "id": "5b867491", "cell_type": "code", "source": "# Set up watsonx model and parameters\nmodel_type = \"meta-llama/llama-2-70b-chat\"\n# model_type = \"google/flan-t5-xxl\"\n# model_type = \"ibm/granite-13b-chat-v1\"\n# model_type = \"ibm/granite-13b-instruct-v1\"\n# model_id = \"ibm/mpt-7b-instruct2\"\nmax_tokens = 100\nmin_tokens = 50\ndecoding = DecodingMethods.GREEDY\ntemperature = 0.7\n\n# Get the watsonx model\nmodel = get_model(model_type, max_tokens, min_tokens, decoding, temperature)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d79b8207", "cell_type": "markdown", "source": "## 5. Answer Generation\n\nThis block generates the answer based on the input prompt, the specified parameters, and above all the specified Watson Discovery collection of data.\n"}, {"metadata": {}, "id": "1e049cc7", "cell_type": "code", "source": "# Send a prompt to model\ngenerated_response = model.generate(prompt)\nresponse_text = generated_response['results'][0]['generated_text']\n\n# Print model response\nprint(\"--------------------------------- Generated response -----------------------------------\")\nprint(response_text)\nprint(\"*********************************************************************************************\")", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}