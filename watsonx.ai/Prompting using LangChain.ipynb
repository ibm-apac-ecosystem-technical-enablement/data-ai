{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d075d",
   "metadata": {},
   "source": [
    "# Hands-on: Prompting using LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this hands-on, you will learn about prompting using LangChain, a framework for building LLM applications.\n",
    "You will learn about:\n",
    "1. Simple Prompt to LLM using LangChain\n",
    "2. Zero-Shot Prompt and Few-Shot Prompt using Prompt Template\n",
    "3. Sequential Prompts using Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c33201",
   "metadata": {},
   "source": [
    "## 1. Simple Prompt to LLM (Manually created Prompt Template)\n",
    "- Basic use case of sending prompts to LLM in watsonx (without using Langchain). \n",
    "- In this example, we are sending a simple prompt directly to the LLM model (Google flan-ul2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664a584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install chromadb==0.4.2\n",
    "!pip install langchain==0.0.312\n",
    "!pip install langchain --upgrade\n",
    "!pip install flask-sqlalchemy --user\n",
    "!pip install pypdf \n",
    "!pip install sentence-transformers\n",
    "!pip install langchain_openai\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-ibm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adcdb35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from langchain import PromptTemplate # Langchain Prompt Template\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96339420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set IBM Cloud API key and Project ID\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "api_key = \"<YOUR IBM CLOUD API KEY HERE>\"\n",
    "project_id = \"<YOUR PROJECT ID HERE>\"\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the watsonx model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"Done initializing LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b25c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a simple prompt to model\n",
    "countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "\n",
    "try:\n",
    "  for country in countries:\n",
    "    question = f\"What is the capital of {country}\"\n",
    "    res = llm_model.generate_text(question)\n",
    "    print(f\"The capital of {country} is {res.capitalize()}\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf74712",
   "metadata": {},
   "source": [
    "## 2. Zero-Shot Prompt and Few-Shot Prompt using LangChain's Prompt Template\n",
    "- Real use case can be more complex. Instead of sending plain prompts to LLM, we are using Langchain Prompt Template. \n",
    "- In this example, we are using Langchain Prompt Template to send prompt to the LLM model (Google flan-ul2).\n",
    "- Advantags of using Prompt Template:\n",
    "    1. **Modularity**: With a prompt template, you can define a structured template once and reuse it with different input variables. This makes your code more modular and easier to maintain.\n",
    "    2. **Dynamic Input**: Prompt templates allow for dynamic input variables, such as \"country\" in this example. This means you can easily change the input value without modifying the entire prompt structure.\n",
    "    3. **Readability**: The prompt template provides a clear and well-defined structure for the prompt, which is maintained separately from the business logic. This separation makes it easier for other developers to understand the purpose of the prompt and how it interacts with the model. Decoupling the prompt creation logic from the core functionality enhances the overall clarity and maintainability of the code.\n",
    "    4. **Flexibility**: You can customize the template to suit your specific use case or domain requirements. This flexibility enables you to adapt the prompt to different scenarios without rewriting the entire prompt logic.\n",
    "    5. **Scalability**: Langchainâ€™s templating system allows for easy management of multiple prompts, which can be reused and adapted as needed without duplicating code.\n",
    "    6. **Maintainability**: The prompt template can be updated in a single place, reducing the risk of errors and making it easier to maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92d3c2",
   "metadata": {},
   "source": [
    "### 2.1 Zero-shot Prompt\n",
    "- Zero-shot prompt is the simplest type of prompt. It provides no examples to the model, just the instruction. \n",
    "- You can phrase the instruction as a question. i.e: *\"Explain the concept of Generative AI.\"*\n",
    "- You can also give the model a 'role'. i.e: *\"You are a Data Scientist. Explain the concept of Generative AI.\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b140fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = [\"country\"],\n",
    "  template = \"What is the capital of {country}?\",\n",
    ")\n",
    "\n",
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    response = chain.run(country)\n",
    "    print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5340",
   "metadata": {},
   "source": [
    "### 2.2 Few-shot Prompt\n",
    "- Few-shot prompt is giving the model a few examples to figure out how to handle similar task in the future.\n",
    "- It helps the model understand the task better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572caefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot examples\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57575ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2d56c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# Few -shot examples\n",
    "examples = [\n",
    "    {\"input\": \"What is the capital of Sweden?\", \"output\": \"The capital of Sweden is Stockholm\"},\n",
    "    {\"input\": \"What is the capital of Malaysia?\", \"output\": \"The capital of Malaysia is Kuala Lumpur\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('human', '{input}'), ('ai', '{output}')]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        #('system', 'You are a helpful AI Assistant'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # In order to use Langchain, we need to instantiate Langchain extension\n",
    "  lc_llm_model = WatsonxLLM(model=llm_model)\n",
    "  \n",
    "  # Define a chain based on model and prompt\n",
    "  chain = LLMChain(llm=lc_llm_model, prompt=final_prompt)\n",
    "\n",
    "  # Getting predictions\n",
    "  countries = [\"France\", \"Japan\", \"Australia\"]\n",
    "  for country in countries:\n",
    "    prompt = f\"What is the capital of {country}?\"\n",
    "    print(prompt)\n",
    "    response = chain.run(prompt)\n",
    "    print(response)\n",
    "    #print(prompt.format(country=country) + \" = \" + response.capitalize())\n",
    "    sleep(0.5)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4674b8",
   "metadata": {},
   "source": [
    "## 3. Sequential Prompts using Simple Sequential Chain\n",
    "- By using Simple Sequential Chain in LangChain, you can easily chain multiple prompts to create sequential prompts.\n",
    "- Prompt chaining, also known as Sequential prompts, enables the response to one prompt to become the input for the next prompt in the sequence.\n",
    "- Each subsequent prompt is informed by the AI's previous response, creating a chain of interactions that progressively refines the model's output.\n",
    "- Reference: [SimpleSequentialChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.sequential.SimpleSequentialChain.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e999911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sequential prompts \n",
    "pt1 = PromptTemplate(input_variables=[\"topic\"], template=\"Generate a random question about {topic}: Question: \")\n",
    "pt2 = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer the following question: {question}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4aabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate 2 models (Note, these could be different models depending on use case)\n",
    "# Note the .to_langchain() method which returns a WatsonxLLM wrapper, like above.\n",
    "model_1 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "model_2 = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833f356c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the sequential chain\n",
    "prompt_to_model_1 = LLMChain(llm=model_1, prompt=pt1)\n",
    "prompt_to_model_2 = LLMChain(llm=model_2, prompt=pt2)\n",
    "qa = SimpleSequentialChain(chains=[prompt_to_model_1, prompt_to_model_2], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6495db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run our chain with the topic: \"an animal\"\n",
    "# Play around with providing different topics to see the output. eg. cars, the Roman empire\n",
    "try:\n",
    "  qa.run(\"an animal\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
