{"cells": [{"metadata": {}, "id": "c64d075d", "cell_type": "markdown", "source": "# Hands-on: Build AI Apps with RAG using watsonx.ai, LangChain & Vector Database\n\n## Overview\n\nIn this hands-on, you will use LangChain, a framework for building LLM applications.\n\n\nRAG (Retrieval Augmented Generation) enables LLMs to interact with external data, crucial for proprietary company information. While there are varied RAG implementations, this lab focuses on LangChain's RetrievalQA API, tailored for question-and-answer tasks. RetrievalQA utilizes an in-memory vector database, ideal for small documents and prototyping but may not scale well for large datasets.\n\nYou will learn how to use LangChain to demo these 2 use cases:\n1. Retrieval Question Answering (QA)\n2. Documents Summarization"}, {"metadata": {}, "id": "ffd84ba9", "cell_type": "markdown", "source": "## 1. Retrieval Question Answering (QA)\n- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question).\n- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n- Then, upload your file to Project and create the access token."}, {"metadata": {"scrolled": true}, "id": "7f62b4bb", "cell_type": "code", "source": "# Install libraries\n!pip install chromadb==0.4.2\n!pip install langchain==0.0.312\n!pip install langchain --upgrade\n!pip install flask-sqlalchemy --user\n!pip install pypdf \n!pip install sentence-transformers\n!pip install langchain_openai", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "207b0b60", "cell_type": "code", "source": "# Import libraries\nimport os\nimport warnings\n\n#from dotenv import load_dotenv\nfrom time import sleep\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\nfrom ibm_watson_studio_lib import access_project_or_space\nfrom langchain import PromptTemplate # Langchain Prompt Template\nfrom langchain.chains import LLMChain, SimpleSequentialChain # LangChain Chains\nfrom langchain.chains import RetrievalQA # LangChain Retrieval QA\nfrom langchain.document_loaders import PyPDFLoader\nfrom langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\nfrom langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\nfrom langchain.text_splitter import CharacterTextSplitter # Text splitter\n\nwarnings.filterwarnings(\"ignore\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "9192335d", "cell_type": "code", "source": "# Set IBM Cloud API key and Project ID\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\napi_key = \"<YOUR IBM CLOUD API KEY HERE>\"\nproject_id = \"<YOUR PROJECT ID HERE>\"\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    raise Exception(\"One or more environment variables are missing!\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "ba95a162", "cell_type": "code", "source": "# Initialize the watsonx model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 25,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 20\n}\n\nllm_model = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n)\n\nprint(\"Done initializing LLM.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "edc8757d", "cell_type": "code", "source": "# Create access token in project\ntoken = \"<YOUR ACCESS TOKEN HERE>\"\nwslib = access_project_or_space({\"token\":token})\nwslib.download_file(\"what_is_generative_ai.pdf\")", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "id": "839d0893", "cell_type": "code", "source": "# Load PDF document\npdf = 'what_is_generative_ai.pdf'\nloaders = [PyPDFLoader(pdf)]", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "id": "62bb2f6e", "cell_type": "code", "source": "# Index loaded PDF\nindex = VectorstoreIndexCreator(\n    embedding = HuggingFaceEmbeddings(),\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "e723ff88", "cell_type": "code", "source": "# Initialize watsonx google/flan-ul2 model\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.2,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 100,\n    GenParams.MIN_NEW_TOKENS: 50,\n    GenParams.MAX_NEW_TOKENS: 300\n}\n\nmodel = Model(\n    model_id=\"google/flan-ul2\",\n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "997ae488", "cell_type": "code", "source": "# Initialize RAG chain\nchain = RetrievalQA.from_chain_type(llm=model, \n                                    chain_type=\"stuff\", \n                                    retriever=index.vectorstore.as_retriever(), \n                                    input_key=\"question\")", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "d4891cb0", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What is Machine Learning?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "20564da9", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the problems generative AI can solve?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "a3ab61fd", "cell_type": "code", "source": "# Answer based on the document\nres = chain.run(\"What are the risks of Generative AI?\")\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "7187e120", "cell_type": "markdown", "source": "## 2. Documents Summarization\n- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n- In this example, we will summarize long documents from these 3 websites:\n     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n- When building a summarizer app, these are methods to pass your documents into the LLM\u2019s context window:\n    1. **Method 1: Stuff** - Simply \u201cstuff\u201d all documents into a single prompt. (Simplest method)\n    2. **Method 2: MapReduce** - Summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary."}, {"metadata": {"scrolled": true}, "id": "c551b02c", "cell_type": "code", "source": "# Install library\n!pip3 install transformers chromadb langchain", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "b8903f79", "cell_type": "code", "source": "# Import libraries\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import WebBaseLoader\nfrom langchain.chains.summarize import load_summarize_chain\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "0eaaddb9", "cell_type": "markdown", "source": "### 2.1 Method 1: Stuff\n- This method simply \u201cstuff\u201d all documents into a single prompt.\n- What you need to do is setting `stuff` as `chain_type` of your chain."}, {"metadata": {}, "id": "915fcefa", "cell_type": "markdown", "source": "### Stuff without using Prompt Template\n- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n- Set `stuff` as the `chain_type`.\n- In this example, you will see that the relatively short document will be summarized successfully."}, {"metadata": {"scrolled": false}, "id": "3dba2d0a", "cell_type": "code", "source": "# Initialize document loader\nloader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\ndoc = loader.load()\n\n# Initialize watsonx google/flan-t5-xxl model\n# You might need to tweak some of the runtime parameters to optimize the results\nparams = {\n    GenParams.DECODING_METHOD: \"sample\",\n    GenParams.TEMPERATURE: 0.15,\n    GenParams.TOP_P: 1,\n    GenParams.TOP_K: 20,\n    GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 20,\n    GenParams.MAX_NEW_TOKENS: 205\n}\n\nflan_model = Model(\n    model_id=\"google/flan-t5-xxl\", \n    params=params,\n    credentials=creds,\n    project_id=project_id\n).to_langchain()\n\n# Set chain_type as 'stuff'\nchain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n\n# Run summarization task\nres = chain.run(doc)\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "eeb53a81", "cell_type": "markdown", "source": "### Stuff using Prompt Template\n- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n- In this example, you will see the same summarization output as above.\n- Reference: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"}, {"metadata": {"scrolled": true}, "id": "6b1c8e1a", "cell_type": "code", "source": "#Import librararies\nfrom langchain.chains.llm import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.combine_documents.stuff import StuffDocumentsChain\n\n# Define prompt\nprompt_template = \"\"\"Write a concise summary of the following:\n\"{text}\"\nCONCISE SUMMARY:\"\"\"\nprompt = PromptTemplate.from_template(prompt_template)\n\n# Define LLMs chain\nllm_chain = LLMChain(llm=flan_model, prompt=prompt)\n\n# Define StuffDocumentsChain\nstuff_chain = StuffDocumentsChain(\n    llm_chain=llm_chain, document_variable_name=\"text\"\n)\n\n# Run summarization task \nres = stuff_chain.run(doc)\nprint(res)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "2938eb51", "cell_type": "markdown", "source": "### Limitation of 'Stuff' Method due to LLMs token limit\n- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n- This is due to the token limit for the model (Max context window length). \n- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."}, {"metadata": {}, "id": "43c2879b", "cell_type": "code", "source": "# Load a new document from URL\nloader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\ndoc_2 = loader_2.load()\n\n# Combine the new document to the previous document\ndocs = doc + doc_2\n\n# Run the stuff chain\ntry:\n  res = stuff_chain.run(docs)\n  print(res)\nexcept Exception as e:\n  print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "26d3f5e1", "cell_type": "markdown", "source": "### 2.2 Method 2: MapReduce\n- This method summarize each document on it\u2019s own in a \u201cmap\u201d step and then \u201creduce\u201d the summaries into a final summary.\n- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"}, {"metadata": {}, "id": "afc5688e", "cell_type": "code", "source": "from transformers import AutoTokenizer\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\nfrom time import perf_counter\n\n# Add a 3rd document\nprint(\"Loading 3rd document...\")\nloader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\ndoc_3 = loader_3.load()\ndocs = docs + doc_3\n\n# Map\nmap_template = \"\"\"The following is a set of documents\n{docs}\nBased on this list of docs, please identify the main themes \nHelpful Answer:\"\"\"\nmap_prompt = PromptTemplate.from_template(map_template)\nprint(\"Init map chain...\")\nmap_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n\n# Reduce\nreduce_template = \"\"\"The following is set of summaries:\n{doc_summaries}\nTake these and distill it into a final, consolidated summary of the main themes. \nHelpful Answer:\"\"\"\nreduce_prompt = PromptTemplate.from_template(reduce_template)\nprint(\"Init reduce chain...\")\nreduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n\n# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\nprint(\"Stuff documents using reduce chain...\")\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n)\n\n# Combines and iteravely reduces the mapped documents\nreduce_documents_chain = ReduceDocumentsChain(\n    # This is final chain that is called.\n    combine_documents_chain=combine_documents_chain,\n    # If documents exceed context for `StuffDocumentsChain`\n    collapse_documents_chain=combine_documents_chain,\n    # The maximum number of tokens to group documents into.\n    token_max=4000\n)\n\n# Combining documents by mapping a chain over them, then combining results\nmap_reduce_chain = MapReduceDocumentsChain(\n    # Map chain\n    llm_chain=map_chain,\n    # Reduce chain\n    reduce_documents_chain=reduce_documents_chain,\n    # The variable name in the llm_chain to put the documents in\n    document_variable_name=\"docs\",\n    # Return the results of the map steps in the output\n    return_intermediate_steps=True,\n    verbose=True\n)\n\n# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n# You might want to play around with different tokenizers and text splitters to see how the results change.\nprint(\"Init chunk splitter...\")\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer=tokenizer\n    )\n    split_docs = text_splitter.split_documents(docs)\n    print(f\"Using {len(split_docs)} chunks: \")\nexcept Exception as ex:\n    print(ex)\n\nprint(\"Run map-reduce chain. This should take ~15-30 seconds...\")\ntry:\n    t1_start = perf_counter()\n    results = map_reduce_chain(split_docs)\n    steps = results[\"intermediate_steps\"]\n    output = results[\"output_text\"]\n    t1_stop = perf_counter()\n    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n\n    print(\"Results from each chunk: \\n\")\n    for idx, step in enumerate(steps):\n        print(f\"{idx + 1}. {step}\\n\")\n    \n    print(\"\\n\\nFinal output:\\n\")\n    print(output)\n\n    print(\"\\nDone.\")\nexcept Exception as e:\n    print(e)", "execution_count": null, "outputs": []}, {"metadata": {}, "id": "240c73c2", "cell_type": "markdown", "source": "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 5}