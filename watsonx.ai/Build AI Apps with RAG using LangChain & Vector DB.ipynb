{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64d075d",
   "metadata": {},
   "source": [
    "# Hands-on: Build AI Apps with RAG using watsonx.ai, LangChain & Vector Database\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this hands-on, you will use LangChain, a framework for building LLM applications.\n",
    "\n",
    "\n",
    "RAG (Retrieval Augmented Generation) enables LLMs to interact with external data, crucial for proprietary company information. While there are varied RAG implementations, this lab focuses on LangChain's RetrievalQA API, tailored for question-and-answer tasks. RetrievalQA utilizes an in-memory vector database, ideal for small documents and prototyping but may not scale well for large datasets.\n",
    "\n",
    "You will learn how to use LangChain to demo these 2 use cases:\n",
    "1. Retrieval Question Answering (QA)\n",
    "2. Documents Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd84ba9",
   "metadata": {},
   "source": [
    "## 1. Retrieval Question Answering (QA)\n",
    "- Using Retrieval Question Answering (QA) in LangChain, you can easily extract passages from documents as answers to your prompt (Question).\n",
    "- To begin, download a sample pdf file from this link: [what_is_generative_ai.pdf](https://ibm.box.com/v/what-is-generative-ai)\n",
    "- Then, upload your file to Project and create the access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62b4bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install chromadb==0.4.2\n",
    "!pip install langchain==0.0.312\n",
    "!pip install langchain --upgrade\n",
    "!pip install flask-sqlalchemy --user\n",
    "!pip install pypdf \n",
    "!pip install sentence-transformers\n",
    "!pip install langchain_openai\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-ibm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207b0b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_studio_lib import access_project_or_space\n",
    "from langchain import PromptTemplate # Langchain Prompt Template\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # LangChain Chains\n",
    "from langchain.chains import RetrievalQA # LangChain Retrieval QA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator # Vectorize db index with chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # For using HuggingFace embedding models\n",
    "from langchain.text_splitter import CharacterTextSplitter # Text splitter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set IBM Cloud API key and Project ID\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "api_key = \"<YOUR IBM CLOUD API KEY HERE>\"\n",
    "project_id = \"<YOUR PROJECT ID HERE>\"\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba95a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the watsonx model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 25,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 20\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"Done initializing LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc8757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create access token in project\n",
    "token = \"<YOUR ACCESS TOKEN HERE>\"\n",
    "wslib = access_project_or_space({\"token\":token})\n",
    "wslib.download_file(\"what_is_generative_ai.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839d0893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load PDF document\n",
    "pdf = 'what_is_generative_ai.pdf'\n",
    "loaders = [PyPDFLoader(pdf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb2f6e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Index loaded PDF\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding = HuggingFaceEmbeddings(),\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e723ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize watsonx google/flan-ul2 model\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.2,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.MAX_NEW_TOKENS: 300\n",
    "}\n",
    "\n",
    "model = Model(\n",
    "    model_id=\"google/flan-ul2\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997ae488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG chain\n",
    "chain = RetrievalQA.from_chain_type(llm=model, \n",
    "                                    chain_type=\"stuff\", \n",
    "                                    retriever=index.vectorstore.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4891cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What is Machine Learning?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20564da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What are the problems generative AI can solve?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ab61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer based on the document\n",
    "res = chain.run(\"What are the risks of Generative AI?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187e120",
   "metadata": {},
   "source": [
    "## 2. Documents Summarization\n",
    "- Text summarization is a task in NLP that makes short but informative summaries of long texts. LLM can be used to make summaries of news articles, research papers, technical documents, and other kinds of text.\n",
    "- Summarizing long documents can be challenging. To generate summaries, you need to apply summarization strategies on your indexed documents. \n",
    "- In this example, we will summarize long documents from these 3 websites:\n",
    "     - https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\n",
    "     - https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/\n",
    "     - https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\n",
    "- When building a summarizer app, these are methods to pass your documents into the LLM’s context window:\n",
    "    1. **Method 1: Stuff** - Simply “stuff” all documents into a single prompt. (Simplest method)\n",
    "    2. **Method 2: MapReduce** - Summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c551b02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install library\n",
    "!pip3 install transformers chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8903f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaaddb9",
   "metadata": {},
   "source": [
    "### 2.1 Method 1: Stuff\n",
    "- This method simply “stuff” all documents into a single prompt.\n",
    "- What you need to do is setting `stuff` as `chain_type` of your chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915fcefa",
   "metadata": {},
   "source": [
    "### Stuff without using Prompt Template\n",
    "- Prompt and LLMs pipeline is wrapped in a single object: `load_summarize_chain`.\n",
    "- Set `stuff` as the `chain_type`.\n",
    "- In this example, you will see that the relatively short document will be summarized successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dba2d0a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize document loader\n",
    "loader = WebBaseLoader(\"https://www.ibm.com/blog/what-can-ai-and-generative-ai-do-for-governments/\")\n",
    "doc = loader.load()\n",
    "\n",
    "# Initialize watsonx google/flan-t5-xxl model\n",
    "# You might need to tweak some of the runtime parameters to optimize the results\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"sample\",\n",
    "    GenParams.TEMPERATURE: 0.15,\n",
    "    GenParams.TOP_P: 1,\n",
    "    GenParams.TOP_K: 20,\n",
    "    GenParams.REPETITION_PENALTY: 1.0,\n",
    "    GenParams.MIN_NEW_TOKENS: 20,\n",
    "    GenParams.MAX_NEW_TOKENS: 205\n",
    "}\n",
    "\n",
    "flan_model = Model(\n",
    "    model_id=\"google/flan-t5-xxl\", \n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ").to_langchain()\n",
    "\n",
    "# Set chain_type as 'stuff'\n",
    "chain = load_summarize_chain(flan_model, chain_type=\"stuff\")\n",
    "\n",
    "# Run summarization task\n",
    "res = chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb53a81",
   "metadata": {},
   "source": [
    "### Stuff using Prompt Template\n",
    "- You will load the document into a prompt template and run a \"stuffed document chain\". Note that we can stuff a list of documents as well.\n",
    "- `StuffDocumentsChain` will be used as part of the `load_summarize_chain` method.\n",
    "- In this example, you will see the same summarization output as above.\n",
    "- Reference: [StuffDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.StuffDocumentsChain.html#langchain.chains.combine_documents.stuff.StuffDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1c8e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import librararies\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "# Define prompt\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\"{text}\"\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define LLMs chain\n",
    "llm_chain = LLMChain(llm=flan_model, prompt=prompt)\n",
    "\n",
    "# Define StuffDocumentsChain\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain, document_variable_name=\"text\"\n",
    ")\n",
    "\n",
    "# Run summarization task \n",
    "res = stuff_chain.run(doc)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938eb51",
   "metadata": {},
   "source": [
    "### Limitation of 'Stuff' Method due to LLMs token limit\n",
    "- In this example, you will see that as we add more documents (which increase the tokens), this error will be raised: `the number of input tokens 5222 cannot exceed the total tokens limit 4096 for this model`\n",
    "- This is due to the token limit for the model (Max context window length). \n",
    "- With LangChain, this can be worked around by using `MapReduce` which execute chunking and recursive summarization method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a new document from URL\n",
    "loader_2 = WebBaseLoader('https://www.govexec.com/technology/2023/07/what-will-federal-government-do-generative-ai/388595/')\n",
    "doc_2 = loader_2.load()\n",
    "\n",
    "# Combine the new document to the previous document\n",
    "docs = doc + doc_2\n",
    "\n",
    "# Run the stuff chain\n",
    "try:\n",
    "  res = stuff_chain.run(docs)\n",
    "  print(res)\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3f5e1",
   "metadata": {},
   "source": [
    "### 2.2 Method 2: MapReduce\n",
    "- This method summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary.\n",
    "- Reference: [ReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.reduce.ReduceDocumentsChain.html#langchain.chains.combine_documents.reduce.ReduceDocumentsChain)\n",
    "- Reference: [MapReduceDocumentsChain](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain.html#langchain.chains.combine_documents.map_reduce.MapReduceDocumentsChain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc5688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from time import perf_counter\n",
    "\n",
    "# Add a 3rd document\n",
    "print(\"Loading 3rd document...\")\n",
    "loader_3 = WebBaseLoader(\"https://www.thomsonreuters.com/en-us/posts/government/ai-use-government-agencies/\")\n",
    "doc_3 = loader_3.load()\n",
    "docs = docs + doc_3\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=flan_model, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=flan_model, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Note here we are using a pretrained tokenizer from Huggingface, specifically for the flan-ul2 model.\n",
    "# You might want to play around with different tokenizers and text splitters to see how the results change.\n",
    "print(\"Init chunk splitter...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\") # Hugging face tokenizer for flan-ul2\n",
    "    text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Using {len(split_docs)} chunks: \")\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print(\"Run map-reduce chain. This should take ~15-30 seconds...\")\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(split_docs)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    print(\"Results from each chunk: \\n\")\n",
    "    for idx, step in enumerate(steps):\n",
    "        print(f\"{idx + 1}. {step}\\n\")\n",
    "    \n",
    "    print(\"\\n\\nFinal output:\\n\")\n",
    "    print(output)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c73c2",
   "metadata": {},
   "source": [
    "- As you can see, Langchain along with a tokenizer for the model can quickly divide a larger amount of text into chunks and recursively summarize into a concise sentence or two. You might want to play around with trying different documents, tweaking the model runtime parameters, and trying a different model alltogether to see how things behave. One of the most important things to note in order to get good results is that the way the input is chunked and tokenized matters a lot. Passing poor map results will result in a lower quality summarization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
