{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "# Hands-on: Implement Memory with LangChain\n\n## Overview\n\nIn this hands-on, you will implement memory using LangChain.\n\nFor some use cases, specifically for the ones that involve multiple interactions with an LLM (for example, a chat) we may need to maintain a \u201chistory\u201d of the conversation, which in programming terms is described as memory.\n\nThe benefit of this approach is that it\u2019s easy to understand and implement. However, there are\ntwo important considerations:\n- The token limit of LLMs still applies. If you keep adding prompts and responses to\nmemory, you can quickly run out of tokens.\n- Cost of tokens if you\u2019re using a hosted instance of LLMs.\n\nLangChain provides several types of memory to help mitigate these issues, such as:\n- `ConversationBufferWindowMemory`: Keeps a list of the interactions in the conversation\nover time. It only uses the last K interactions.\n- `ConversationSummaryBufferMemory`: Keeps a summary of interactions, using token length\nrather than number of interactions to determine when to flush interactions.\n- `ConversationTokenBufferMemory`: Keeps a buffer of recent interactions in memory, and\nuses token length rather than number of interactions to determine when to flush\ninteractions.\n\nThis notebook contains sample code for using `ConversationBufferMemory` with LLM included in watsonx.ai.\nAt the time of writing this lab, WML API supports integration with the most basic memory type, `ConversationBufferMemory`, which keeps the entire conversation in memory until the memory fills or until it\u2019s explicitly cleared."}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "# Install libraries\n!pip install pip install ibm-watson-machine-learning --upgrade\n!pip install langchain | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import libraries\nfrom ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\nfrom ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chains import SimpleSequentialChain\n\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Set IBM Cloud API key and Project ID\nibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\"\napi_key = \"<YOUR IBM CLOUD API KEY HERE>\"\nproject_id = \"<YOUR PROJECT ID HERE>\"\n\nif api_key is None or ibm_cloud_url is None or project_id is None:\n    raise Exception(\"One or more environment variables are missing!\")\nelse:\n    creds = {\n        \"url\": ibm_cloud_url,\n        \"apikey\": api_key \n    }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Initialize the watsonx model\n\nmodel_id = 'ibm-mistralai/mixtral-8x7b-instruct-v01-q'\n\nparams = {\n    GenParams.DECODING_METHOD: 'greedy',\n    #GenParams.TEMPERATURE: 0.2,\n    #GenParams.TOP_P: 1,\n    #GenParams.TOP_K: 25,\n    #GenParams.REPETITION_PENALTY: 1.0,\n    GenParams.MIN_NEW_TOKENS: 1,\n    GenParams.MAX_NEW_TOKENS: 300\n}\n\nllm_model = Model(\n    model_id=model_id,\n    params=params,\n    credentials=creds,\n    project_id=project_id\n)\n\n# In order to use Langchain, we need to instantiate Langchain extension\nlc_llm_model = WatsonxLLM(model=llm_model)\n\nprint(\"Done initializing LLM.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create the memory object\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=lc_llm_model, \n    memory = memory,\n    verbose=True\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Example of a first quesiton from the user\nuser_input = \"From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \\\n                            Put each factor on a new line. Customer complaint: I am writing you this statement to delete the \\\n                            following information on my credit report. The items I need deleted are listed in the report. \\\n                            I am a victim of identity theft, I demand that you remove these errors to correct my report immediately! \\\n                            I have reported this to the federal trade commission and have attached the federal trade commission affidavit. \\\n                            Now that I have given you the following information, please correct my credit report or I shall proceed with involving my attorney! \\\n                            Numbered list of complaints:\"\n\n#user_input=\"Right now I am bothered! I have attempted to be patient however it is hard to be patient when you feel that you are continually being overlooked by somebody. I think you fail to remember that \\Consumer detailing organizations have expected an essential part in amassing and assessing customer credit and other data on shoppers. The XXXX XXXX  is reliant on the reasonable and precision.\"\n\n# Invoke the LLM\nconversation.predict(input=user_input)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Example of a second quesiton from the user\nuser_input = \"Does the list of complaints contain a statement about identity fraud? Provide a short answer: yes or no.\"\nconversation.predict(input=user_input)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# For debugging, print the history of the conversation\nprint(memory.buffer)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Run this if you want to clear the memory buffer\nmemory.clear()\nprint(memory.buffer)", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}