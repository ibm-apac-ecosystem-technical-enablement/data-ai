{"cells":[{"cell_type":"markdown","id":"72e30227-4902-4500-a659-b79717befdb5","metadata":{"id":"f7e28a78-d158-40c1-92de-56937f84ad7d"},"source":["# Retrieval and Answer Quality Metrics computation using LLM as Judge in IBM watsonx.governance for RAG task\n","\n","This notebook demonstrates the creation of Retrieval Augumented Generation pattern using watsonx.ai and computation of reference free Retrieval Quality metric **Context relevance** and Answer Quality metrics such as **Faithfulness**, **Answer relevance** and reference based **Answer similarity** metric for RAG task type using LLM and IBM watsonx.governance.\n","\n","**Context relevance** assesses the degree to which the retrieved context is relevant with the question specified in the prompt, serving as a metric for evaluating the quality of your retrieval system. The context relevance score is a value between 0 and 1. A value closer to 1 indicates that the context is more relevant to your question in the prompt. A value closer to 0 indicates that the context is less relevant to your question in the prompt.\n","\n","**Faithfulness** measures how faithful the answer or generated text is to the context sent to the LLM input. The faithfulness score is a value between 0 and 1. A value closer to 1 indicates that the output is more faithful or grounded and less hallucinated. A value closer to 0 indicates that the output is less faithful or grounded and more hallucinated.\n","\n","**Answer relevance** measures how relevant the answer or generated text is to the question. This is one of the ways to determine the quality of your model. The answer relevance score is a value between 0 and 1. A value closer to 1 indicates that the answer is more relevant to the given question. A value closer to 0 indicates that the answer is less relevant to the question.\n","\n","**Answer similarity** measures how similar the answer or generated text is to the ground truth or reference answer. This is one of the ways to determine the quality of your model. The answer similarity score is a value between 0 and 1. A value closer to 1 indicates that the answer is more similar to the reference value. A value closer to 0 indicates that the answer is less similar to the reference value."]},{"cell_type":"markdown","id":"a8b45bee","metadata":{},"source":["## Learning goals\n","\n","- Ingest data into a vector database\n","- Initialize foundation model\n","- Generate RAG responses\n","- Configure and run evaluations"]},{"cell_type":"markdown","id":"b0a5d37f","metadata":{},"source":["## Contents\n","\n","- [Step 1 - Setup](#setup)\n","- [Step 2 - Ingest Data into Vector DB](#data)\n","- [Step 3 - Initialize foundational model using watsonx.ai](#model)\n","- [Step 4 - Generate the answers to questions using LangChain RetrievalQA](#predict)\n","- [Step 5 - Configure evaluations](#config)\n","- [Step 6 - Run evaluations](#compute)\n","- [Step 7 - Display the results](#results)\n","- [Challenge - Side-by-side comparison between IBM Granite and llama 3 Models](#challenge)"]},{"cell_type":"markdown","id":"b48e55a5-6a38-4cce-a043-990da6cadb6e","metadata":{},"source":["## Step 1 - Setup <a id=\"setup\"></a>"]},{"cell_type":"markdown","id":"ccd0c957-b76c-4ab9-bba9-467cd78638e5","metadata":{},"source":["### Install necessary libraries"]},{"cell_type":"code","execution_count":null,"id":"a9a66f96","metadata":{"id":"79c58c18-ac59-4cc9-9aa9-f355257b22b2"},"outputs":[],"source":["!pip install -U \"ibm-metrics-plugin~=3.0.0\" | tail -n 1\n","!pip install -U ibm-watson-openscale | tail -n 1\n","!pip install -U ibm-watson-machine-learning | tail -n 1\n","!pip install \"langchain==0.0.345\" | tail -n 1\n","!pip install wget | tail -n 1\n","!pip install sentence-transformers | tail -n 1\n","!pip install \"chromadb==0.3.26\" | tail -n 1\n","!pip install \"pydantic==1.10.0\" | tail -n 1\n","!pip install nltk\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","id":"4d42b608-30ce-4ded-a806-7fc3905d53de","metadata":{},"source":["**Note**: you may need to restart the kernel to use updated libraries."]},{"cell_type":"markdown","id":"d9266c40","metadata":{"id":"da88ac4f-de52-40de-a6fb-da46541b5624"},"source":["### Configure your credentials"]},{"cell_type":"markdown","id":"cf414e50-9fe7-4744-a132-bf3fc91666a2","metadata":{"id":"cf414e50-9fe7-4744-a132-bf3fc91666a2"},"source":["***Hint***: You can generates `CLOUD_API_KEY` by accessing https://cloud.ibm.com/ -> Manage -> API Key -> Create New Key"]},{"cell_type":"code","execution_count":null,"id":"ec7036fa","metadata":{"id":"73c01592-268c-4e5e-985c-89a10e491d89"},"outputs":[],"source":["# Cloud credentials\n","IAM_URL=\"https://iam.cloud.ibm.com\"\n","DATAPLATFORM_URL = \"https://api.dataplatform.cloud.ibm.com\"\n","SERVICE_URL = \"https://aiopenscale.cloud.ibm.com\"\n","CLOUD_API_KEY = \"<EDIT THIS>\" # YOUR_CLOUD_API_KEY\n","\n","credentials = {\n","    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n","    \"apikey\": CLOUD_API_KEY\n","}"]},{"cell_type":"markdown","id":"3c181c4e-2b54-4bf0-9070-223bf7837c76","metadata":{},"source":["### Configure your project id\n","Provide the project id to provide the context needed to run the inference against the watsonx.ai model.\n","\n","***Hint***: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be \"Projects / *project name* /\". Click on the \"*project name*\" link, then get the `project_id` from the project's \"Manage\" tab (\"Project -> Manage -> General -> Details\")."]},{"cell_type":"code","execution_count":null,"id":"de23d9b2-fd37-454a-82d9-c0f37b2766ac","metadata":{"id":"3642be4c-aaa4-4a62-9008-73f32e1a1a10"},"outputs":[],"source":["project_id = \"<EDIT THIS>\" # YOUR_PROJECT_ID"]},{"cell_type":"markdown","id":"0150430a","metadata":{},"source":["## Step 2 - Ingest Data into Vector DB <a id=\"data\"></a>"]},{"cell_type":"markdown","id":"ec65a592-7608-45bb-bae2-bb4cf5fccc88","metadata":{},"source":["### Read the data\n","\n","Download the sample \"State of the Union\" file."]},{"cell_type":"code","execution_count":null,"id":"8d2ddc58-7909-4a03-8089-4261cba5876f","metadata":{"id":"14bcae38-f3ea-492c-92cf-36af4daeabe4"},"outputs":[],"source":["import wget\n","import os\n","\n","data = 'state_of_the_union.txt'\n","url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n","\n","if not os.path.isfile(data):\n","    wget.download(url, out=data)"]},{"cell_type":"markdown","id":"bb70d1af-2750-41de-85b0-815ad3758ee5","metadata":{},"source":["### Prepare the data for the vector database\n","\n","Take the `state_of_the_union.txt` speech content data and split it into chunks. "]},{"cell_type":"code","execution_count":null,"id":"4a64a21d-437e-421a-be84-ff605e02d142","metadata":{"id":"ade08a95-29fc-4ec8-bc15-eb23f873d67e"},"outputs":[],"source":["from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain.vectorstores import Chroma\n","\n","loader = TextLoader(data)\n","documents = loader.load()\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","texts = text_splitter.split_documents(documents)"]},{"cell_type":"markdown","id":"030bbb37-7e24-43d5-821e-a1da1ac7c656","metadata":{},"source":["### Create an embedding function to store the data in a vector database\n","\n","Embed the chunked data using an open-source embedding model and load it into Chromadb, a vector database.\n","\n","**Note**: You can also provide a custom embedding function to be used by Chromadb; the performance of Chromadb may differ depending on the embedding model used."]},{"cell_type":"code","execution_count":null,"id":"6efbbb44-57ca-4c19-949f-756b15a86c41","metadata":{"id":"c13a1981-e4a2-4634-8a2f-ddcddb899492"},"outputs":[],"source":["from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embeddings = HuggingFaceEmbeddings()\n","docsearch = Chroma.from_documents(texts, embeddings)"]},{"cell_type":"markdown","id":"0e9197fe-53ba-4e29-b481-4811859e07ef","metadata":{},"source":["## Step 3 - Initialize a foundation model using `watsonx.ai`\n","<a id=\"model\"></a>"]},{"cell_type":"markdown","id":"f958a4e0-72e7-4683-a952-c769c62ed379","metadata":{},"source":["IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."]},{"cell_type":"markdown","id":"72840462-39cc-4d3a-a39e-acffe2fd8ee3","metadata":{},"source":["### Define the model parameters\n","Provide a set of model parameters that will influence the result:"]},{"cell_type":"code","execution_count":null,"id":"1c65c4db-49aa-493c-82ea-bdf224551f27","metadata":{"id":"0037f7ad-57e5-47c1-a950-8d5a1580c84e"},"outputs":[],"source":["from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n","from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n","\n","parameters = {\n","    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n","    GenParams.MIN_NEW_TOKENS: 1,\n","    GenParams.MAX_NEW_TOKENS: 100,\n","    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n","}"]},{"cell_type":"markdown","id":"f600c734-37d0-47ef-9a5d-8291783d0e79","metadata":{},"source":["### Set LangChain custom LLM wrapper for watsonx model\n","Initialize the `WatsonxLLM` class from LangChain with defined parameters, and using `ibm/granite-13b-chat-v2`. You can find all supported model IDs [here](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-api-model-ids.html?context=wx&audience=wdp)"]},{"cell_type":"code","execution_count":null,"id":"07e1616d-9fce-40d2-a4a1-f2285f04eee3","metadata":{"id":"e7a7b6ff-ab60-448e-abda-502b207df6ad"},"outputs":[],"source":["from langchain.llms import WatsonxLLM\n","\n","watsonx_llm = WatsonxLLM(\n","    model_id='ibm/granite-13b-chat-v2',\n","    url=credentials.get(\"url\"),\n","    apikey=credentials.get(\"apikey\"),\n","    project_id=project_id,\n","    params=parameters\n",")"]},{"cell_type":"markdown","id":"59e5c9e5-079f-44de-acb3-39c691b72114","metadata":{},"source":["## Step 4 - Generate the answers to questions using LangChain RetrievalQA\n","<a id=\"predict\"></a>"]},{"cell_type":"markdown","id":"00bbb5f0-001c-432d-b0ac-5938309b0259","metadata":{},"source":["### Build a `RetrievalQA` (question answering chain) to automate the RAG task."]},{"cell_type":"code","execution_count":null,"id":"06aa4835-e5ec-460a-970f-7db358d20951","metadata":{"id":"bfb67cb0-5c85-46e9-8557-c810d3f81d64"},"outputs":[],"source":["from langchain.chains import RetrievalQA\n","\n","qa = RetrievalQA.from_chain_type(llm=watsonx_llm, chain_type=\"stuff\", retriever=docsearch.as_retriever())"]},{"cell_type":"code","execution_count":null,"id":"35b21c62-6502-45d9-93cc-345dfc441ba2","metadata":{"id":"5ebf9bba-8a3d-47e6-ab3e-08254b086f63"},"outputs":[],"source":["query1 = \"What is ARPA-H?\"\n","query2 = \"What is the investment of Ford and GM to build electric vehicles?\"\n","query3 = \"What is the proposed tax rate for corporations?\"\n","query4 = \"What is Intel going to build?\"\n","query5 = \"How many new manufacturing jobs are created last year?\"\n","query6 = \"How many electric vehicle charging stations are built?\"\n","\n","questions = [query1 , query2, query3, query4, query5, query6]\n","\n","ref_ans1 = \"ARPA-H is the Advanced Research Projects Agency for Health, which is an agency that aims to drive breakthroughs in cancer, Alzheimer's, diabetes, and more. It was proposed by the U.S. President to supercharge the Cancer Moonshot and cut the cancer death rate by at least 50% over the next 25 years.\"\n","ref_ans2 = \"Ford is investing $11 billion to build electric vehicles, creating 11,000 jobs across the country. GM is making the largest investment in its history—$7 billion to build electric vehicles, creating 4,000 jobs in Michigan.So, the total investment of Ford and GM to build electric vehicles is $11 billion + $7 billion = $18 billion.\"\n","ref_ans3 = \"The proposed tax rate for corporations is a 15% minimum tax rate.\"\n","ref_ans4 = \"Intel is going to build a $20 billion semiconductor \\\"mega site\\\" with up to eight state-of-the-art factories.\"\n","ref_ans5 = \"369,000 new manufacturing jobs were created last year.\"\n","ref_ans6 = \"The document does not provide information on the number of electric vehicle charging stations built. It only mentions the plan to build a national network of 500,000 electric vehicle charging stations.\"\n","\n","reference_answers = [ref_ans1, ref_ans2, ref_ans3, ref_ans4, ref_ans5, ref_ans6]"]},{"cell_type":"markdown","id":"c509fc55-43a6-40d6-92fa-04afbe932697","metadata":{},"source":["### Generate retrieval-augmented responses to the questions"]},{"cell_type":"code","execution_count":null,"id":"b0acedda-0754-49bd-a811-099d375f932a","metadata":{"id":"2872dff8-4c70-4cc7-98da-a453f13f4d70"},"outputs":[],"source":["responses = []\n","contexts = []\n","for query in questions:\n","    #Retrive relevant context for each question from the vector db\n","    docs = docsearch.as_retriever().get_relevant_documents(query)\n","\n","    context = []\n","    #Extract the needed information\n","    for doc in docs:\n","        context.append(doc.to_json()['kwargs']['page_content'])\n","\n","    #Capture the context\n","    contexts.append(context)\n","\n","    #Run the prompt and get the response\n","    response = qa.run(query)\n","    responses.append(response)\n","    "]},{"cell_type":"code","execution_count":null,"id":"fb5e60dd-e5cf-4241-8f09-47c855313844","metadata":{"id":"85824f71-fa12-41ed-aced-ace11032ea1b"},"outputs":[],"source":["#Print a sample context retrieved for a query \n","# print(f\"Question:{questions[0]}\\n context:{contexts[0]}\")"]},{"cell_type":"code","execution_count":null,"id":"378b3bfe-ad64-4e6d-8b40-9939a2545800","metadata":{"id":"435c8688-6798-4bdf-aa7d-e59cf0aabcf8"},"outputs":[],"source":["#Print the result\n","# for query in questions:\n","#     print(f\"{query} \\n {responses[questions.index(query)]} \\n\")"]},{"cell_type":"markdown","id":"b2fbd5cc-8847-4b80-8ff7-54dd4938f957","metadata":{},"source":["### Construct a dataframe with question, contexts and answer to be used for metrics computation"]},{"cell_type":"code","execution_count":null,"id":"42686499-4197-48f2-8164-34f5d2b1ce5a","metadata":{"id":"bd985093-cf29-44d8-ba51-648d9144b794"},"outputs":[],"source":["import pandas as pd\n","data = pd.DataFrame(contexts, columns=[\"context1\", \"context2\", \"context3\", \"context4\"])\n","data[\"question\"] = questions\n","data[\"answer\"] = responses\n","data[\"reference\"] = reference_answers"]},{"cell_type":"markdown","id":"360c6ecd","metadata":{"id":"711cecf8-3ae8-4fdd-8e27-30357688b205"},"source":["## Step 5 - Configure Evaluations\n","<a id=\"config\"></a>"]},{"cell_type":"markdown","id":"01cb3495","metadata":{},"source":["### Parameters"]},{"cell_type":"markdown","id":"842541f3-4518-4c5d-a2d9-b06fbe566c22","metadata":{},"source":["#### Common parameters\n","\n","| Parameter | Description | Default Value | Possible Value(s) |\n","|:-|:-|:-|:-|\n","| context_columns | The list of context column names in the input data frame. |  |  |\n","| question_column | the name of the question column in the input data frame. |  |  |\n","| answer_column | The name of the answer column in the input data frame |  |  |\n","| record_level [Optional] | The flag to return the record level metrics values. Set the flag under configuration to generate record level metrics for all the metrics. Set the flag under specific metric to generate record level metrics for that metric alone. | False | True, False |\n","| scoring_fn | The scoring function which takes in the prompts input dataframe and score the LLM acting as Judge, return the output as a dataframe. The input data frame will have a single column \"prompt\" and the output data frame can either have a single column or if there are multiple columns, return the model output text in \"generated_text\" column. | | |\n","\n","### Metric parameters\n","\n","| Parameter | Description | Default Value | Possible Value(s) |\n","|:-|:-|:-|:-|\n","| record_level [Optional] | The flag to return the record level metrics values. Setting the flag under specific metric overrides the value provided at the configuration level. | False | True, False |\n","| metric_prompt_template [Optional] | The prompt template used to compute the metric value. User can override the prompt template used by watsonx.governance to compute the metric using this parameter. The prompt template should use the variables {context}, {question}, {answer}, {reference_answer} as needed and these variable values will be filled with the actual data while calling the scoring function. The prompt response should return the metric value in the range 1-10 for the respective metric and in one of the formats [\"4\", \"7 star\", \"star: 8\", \"stars: 9\"] as answer. | | |"]},{"cell_type":"markdown","id":"8cf2a7e7","metadata":{},"source":["### Verify client version"]},{"cell_type":"code","execution_count":null,"id":"aa0d7844-5c99-40dd-8c03-eb60a3790707","metadata":{"id":"a4b02346-3843-428a-8068-3eb2234d0162"},"outputs":[],"source":["from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n","\n","from ibm_watson_openscale import *\n","from ibm_watson_openscale.supporting_classes.enums import *\n","from ibm_watson_openscale.supporting_classes import *\n","\n","authenticator = IAMAuthenticator(apikey=CLOUD_API_KEY, url=\"https://iam.cloud.ibm.com\")\n","client = APIClient(authenticator=authenticator, service_url=\"https://aiopenscale.cloud.ibm.com\")\n","\n","# print(client.version)"]},{"cell_type":"markdown","id":"1d20ebd8","metadata":{},"source":["### Define the scoring function to invoke the LLM acting as Judge while compute the metrics\n","\n","The scoring function is implemeted using model from watsonx.ai from cloud. The model FLAN_T5_XXL is used as the judge here. The other models which can be used from watsonx.ai are FLAN_UL2, FLAN_T5_XL, MIXTRAL_8X7B_INSTRUCT_V01_Q\n","\n","The function can be changed as needed to invoke external models as well. The quality of the retrieval and answer quality metrics can vary with the model used as judge."]},{"cell_type":"code","execution_count":null,"id":"8d12a66b","metadata":{"id":"8d12a66b"},"outputs":[],"source":["from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n","from ibm_watson_machine_learning.foundation_models import Model\n","from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n","import pandas as pd\n","\n","generate_params = {\n","    GenParams.MAX_NEW_TOKENS: 100,\n","    GenParams.MIN_NEW_TOKENS: 10,\n","    GenParams.TEMPERATURE: 0.0\n","}\n","\n","model = Model(\n","    model_id=ModelTypes.FLAN_T5_XXL,\n","    params=generate_params,\n","    credentials={\n","        \"apikey\": credentials.get(\"apikey\"),\n","        \"url\": credentials.get(\"url\")\n","    },\n","    project_id=project_id\n",")\n","\n","def scoring_fn(data):\n","    results = []\n","    \n","    for prompt_text in data.iloc[:, 0].values.tolist():\n","        model_response = model.generate_text(prompt=prompt_text)\n","        results.append(model_response)\n","    \n","    return pd.DataFrame({\"generated_text\": results})"]},{"cell_type":"markdown","id":"ddb7b684","metadata":{},"source":["### Configure context relevance, faithfulness, answer relevance and answer similarity parameters"]},{"cell_type":"code","execution_count":null,"id":"35bbce5e","metadata":{"id":"c6874bdb-b521-4497-a90d-7ccd864e69f1"},"outputs":[],"source":["from ibm_metrics_plugin.metrics.llm.utils.constants import LLMTextMetricGroup, LLMCommonMetrics, LLMRAGMetrics, RetrievalQualityMetric\n","\n","# Edit below values based on the input data\n","context_columns = [\"context1\", \"context2\", \"context3\", \"context4\"]\n","question_column = \"question\"\n","answer_column = \"answer\"\n","reference_column = \"reference\"\n","\n","config_json = {\n","            \"configuration\": {\n","                \"context_columns\": context_columns,\n","                \"question_column\": question_column,\n","                \"scoring_fn\": scoring_fn,\n","                \"record_level\": True,\n","                LLMTextMetricGroup.RAG.value: {\n","                        LLMRAGMetrics.RETRIEVAL_QUALITY.value: {\n","                            RetrievalQualityMetric.CONTEXT_RELEVANCE.value: {\n","                                #\"record_level\": True,\n","                                #\"metric_prompt_template\": \"\"\n","                            }\n","                        },\n","                        LLMCommonMetrics.FAITHFULNESS.value: {\n","                            #\"record_level\": True,\n","                            #\"metric_prompt_template\": \"\"\n","                        },\n","                        LLMCommonMetrics.ANSWER_RELEVANCE.value: {\n","                            #\"record_level\": True,\n","                            #\"metric_prompt_template\": \"\"\n","                        },\n","                        LLMCommonMetrics.ANSWER_SIMILARITY.value: {\n","                            #\"record_level\": True,\n","                            #\"metric_prompt_template\": \"\"\n","                        }\n","                }\n","            }\n","        }"]},{"cell_type":"markdown","id":"8a7d2879","metadata":{"id":"180bba35-afc7-40a2-ad38-c66fc5201e04"},"source":["### Create the input, output and reference data frames and send them as input to compute metrics"]},{"cell_type":"code","execution_count":null,"id":"436c949c","metadata":{"id":"ca083bc0-8b84-4f7f-ac0e-3a2d56b70a6b"},"outputs":[],"source":["df_input = pd.DataFrame(data, columns=context_columns+[question_column])\n","df_output = pd.DataFrame(data, columns=[answer_column])\n","df_reference = pd.DataFrame(data, columns=[reference_column])"]},{"cell_type":"markdown","id":"60d126ea-7429-4f8e-961b-8ebabae26a2d","metadata":{},"source":["## Step 6 - Run evaluations <a id=\"compute\"></a>"]},{"cell_type":"code","execution_count":null,"id":"683068ae","metadata":{"id":"683068ae"},"outputs":[],"source":["import json\n","metrics_result = client.llm_metrics.compute_metrics(config_json, \n","                                                    sources=df_input, \n","                                                    predictions=df_output,\n","                                                    references=df_reference)\n","\n","# print(json.dumps(metrics_result, indent=2))"]},{"cell_type":"markdown","id":"f32268ba-56ec-43d6-a91d-97c9d59105b5","metadata":{},"source":["## Step 7 - Display the results <a id=\"results\"></a>"]},{"cell_type":"markdown","id":"42dca741-bc78-4550-a5e7-cd33e068e562","metadata":{},"source":["### Get metric results for all records"]},{"cell_type":"code","execution_count":null,"id":"9b4fb1f3-840b-43f9-9e0e-74597f29b0f0","metadata":{"id":"9153b494-f1b3-43aa-b583-18c0fd4899fe"},"outputs":[],"source":["results_df = data.copy()\n","for k, v in metrics_result.items():\n","    metric = \"context_relevance\" if k == \"retrieval_quality\" else k\n","    if v.get(\"record_level_metrics\"):\n","        results_df[metric] = [r.get(metric) for r in v.get(\"record_level_metrics\")]\n","        \n","results_df"]},{"cell_type":"markdown","id":"7b9e558d-c3cc-4a00-aa50-aea2a47c1b45","metadata":{"id":"7b9e558d-c3cc-4a00-aa50-aea2a47c1b45"},"source":["## Challenge - Side-by-side comparison between IBM Granite and llama 3 Models <a id=\"challenge\"></a>"]},{"cell_type":"code","execution_count":null,"id":"ff28f9ca-5efa-4ba4-adfe-3cd6daa5b907","metadata":{"id":"ff28f9ca-5efa-4ba4-adfe-3cd6daa5b907"},"outputs":[],"source":["# [TODO] - side-by-side comparison between IBM Granite and llama 3 models"]},{"cell_type":"markdown","id":"01751f16","metadata":{},"source":["Copyright © 2024 IBM."]}],"metadata":{"kernelspec":{"display_name":"Python 3.11","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
